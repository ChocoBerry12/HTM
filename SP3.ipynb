{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Spatial Pooler\n",
    "- SDR (Sparse Destribution Representation, 희소 분포 표상) 생성\n",
    "- 입력에 대해 pooler 를 이루고 있는 column 들의 일부 (=~2% 내외) 만 활성화 한다\n",
    "- 모든 column 이 비슷한 빈도로 active 되어야함\n",
    "- 모든 column 의 synapse 들이 비슷한 빈도로 active 되어야함\n",
    "\n",
    "\n",
    "- [x] 1-d array column structure\n",
    "- [ ] potential synapse - make tunable, randomly\n",
    "- [ ] hard testing\n",
    "- [ ] visualizing method\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> col : 128, in_size : 32, threshold : 3, perm : 0.8, w : 10, val : 0-30 일 때 안정적인듯. 비슷한 입력에 대해서 유사한 SDR 공간을 공유하고 sparsity 가 일정했고 overlap duty 동 적당했다. 이유는?\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import nbimporter\n",
    "import Encoder\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialPooler:\n",
    "    def __init__(self, input_size, columns=100, conPerm=.5, minOver=5, potential_rate=.8):\n",
    "        self.input_size = input_size                               # input vector 크기\n",
    "        self.input_data = np.empty([self.input_size])\n",
    "        self.columnCount = columns                                 # column 의 가로 크기\n",
    "        self.connectedPerm = conPerm                               # synapse 활성화(1) 될 permanence 임계치\n",
    "        self.min_overlap = minOver                                 # 발화 하기 위한 컬럼 당 최소한의 overlap count\n",
    "        self.minGlobalActivity = 0                                 # 승리하기 위해 필요한 score (global inhibition)\n",
    "        self.desiredGlobalActivity = int(0.05 * self.columnCount)  # 한 번에 승리할 column 수 (global inhibition)\n",
    "        self.minDutyCycle = 0                                      # column 당 최소 발화 duty\n",
    "        self.highDutyCycle = 0\n",
    "        self.permanence_inc = .01                                  # 학습시 permanence 증가량\n",
    "        self.permanence_dec = .01                                  # 학습시 permanence 감소량\n",
    "        self.history_capacity = 100\n",
    "        self.step = 0                                              # 데이터 처리한 수\n",
    "        self.potential_rate = potential_rate                       # 입력 데이터에 대한potential synapses 의 비율\n",
    "        self.potential_count = int(self.input_size*self.potential_rate)\n",
    "        \n",
    "        self.potential_synapses = np.random.random([self.input_size, self.columnCount])   # potential synapses(proxiaml dendrite) - permanence ndarry. 초기화 필요      \n",
    "        self.connected_synapses = np.zeros([self.input_size, self.columnCount])           # 연결된 synapses\n",
    "        self.boosts = np.ones([self.columnCount])                                             # 보정에 필요한 boost \n",
    "        self.overlapped = np.zeros([self.columnCount])                                        # input 과 연결된 synapse 들과의 최초 계산\n",
    "        self.activeColumns = np.zeros([self.columnCount])\n",
    "        self.activeHistory = []                                            # active duty 를 계산하기 위한 active 기록\n",
    "        self.overlapHistory = []                                           # overlap duty 를 계산하기 위한 overlap 기록\n",
    "        self.activeDutyInfo = np.zeros([self.columnCount])                 # active duty 정보\n",
    "        self.overlapDutyInfo = np.zeros([self.columnCount])                # overlap duty 정보\n",
    "        \n",
    "        # potential synapses 구성\n",
    "        \n",
    "        for col in range(self.columnCount):\n",
    "            never_connect = self.input_size - self.potential_count\n",
    "\n",
    "            while(never_connect > 0):\n",
    "                idx = random.randint(0, self.input_size - 1)\n",
    "\n",
    "                if(self.potential_synapses[idx, col] != -1):\n",
    "                    self.potential_synapses[idx, col] = -1\n",
    "                    never_connect -= 1\n",
    "        \n",
    "        ## duty 계산을 위한 history 생성 ##\n",
    "        for c in range(self.columnCount):\n",
    "            self.activeHistory.append(collections.deque())\n",
    "            self.overlapHistory.append(collections.deque())\n",
    "            \n",
    "                \n",
    "    ''' SDR 생성 '''\n",
    "    def compute_SDR(self, input_data):\n",
    "        \n",
    "        self.input_data = input_data\n",
    "        \n",
    "        ## 1. overlaping ##\n",
    "        self.connected_synapses = self.potential_synapses > self.connectedPerm\n",
    "        self.overlapped = self.input_data @ self.connected_synapses\n",
    "        \n",
    "        for c in range(self.columnCount):\n",
    "            if(self.overlapped[c] > self.min_overlap):\n",
    "                self.overlapped[c] *= self.boosts[c]\n",
    "\n",
    "                if(len(self.overlapHistory[c]) >= self.history_capacity):\n",
    "                    self.overlapHistory[c].popleft()\n",
    "\n",
    "                self.overlapHistory[c].append(True)\n",
    "\n",
    "            else:\n",
    "                self.overlapped[c] = 0\n",
    "\n",
    "                if(len(self.overlapHistory[c]) >= self.history_capacity):\n",
    "                    self.overlapHistory[c].popleft()\n",
    "\n",
    "                self.overlapHistory[c].append(False)\n",
    "                    \n",
    "                    \n",
    "        ## 2. inhibition (global) ##\n",
    "        self.minGlobalActivity = self.kthScore(self.desiredGlobalActivity)\n",
    "        self.activeColumns = self.overlapped > self.minGlobalActivity\n",
    "        \n",
    "        for c in range(self.columnCount):                \n",
    "            if(len(self.activeHistory[c]) >= self.history_capacity):\n",
    "                self.activeHistory[c].popleft()\n",
    "\n",
    "            self.activeHistory[c].append(self.activeColumns[c])\n",
    "                    \n",
    "                \n",
    "        ## 3. learning ## \n",
    "        for c in range(self.columnCount):\n",
    "                \n",
    "            if(self.activeColumns[c] == 1):\n",
    "                for s in range(self.input_size):\n",
    "                    \n",
    "                    if(self.potential_synapses[s, c] != -1):\n",
    "                        if(self.input_data[s] == 1):\n",
    "                            self.potential_synapses[s, c] += self.permanence_inc\n",
    "                            self.potential_synapses[s, c] = min(self.potential_synapses[s, c], 1.0)\n",
    "                        else:\n",
    "                            self.potential_synapses[s, c] -= self.permanence_dec\n",
    "                            self.potential_synapses[s, c] = max(0.0, self.potential_synapses[s, c])\n",
    "                                            \n",
    "                    \n",
    "        ## 3.2. 보정 작업 ##\n",
    "        self.update_activeDuty()\n",
    "        self.update_overlapDuty()\n",
    "        self.step += 1\n",
    "        \n",
    "        for c in range(self.columnCount):\n",
    "            ## 자주 승리하지 못하는 column 에 대하여 잘 발화할 수 있도록 boost 시켜줌\n",
    "            maxDuty, highDuty = self.maxhighDutyCycle()\n",
    "            self.minDutyCycle = .1 * maxDuty\n",
    "            self.highDutyCycle = highDuty\n",
    "            #print(\"min :\", self.minDutyCycle)\n",
    "            self.boostFunction(c, .01)\n",
    "            #print(self.boosts)\n",
    "\n",
    "            ## input 과 잘 겹치지 않는 synapse 에 대해서 permanence 증가시켜줌\n",
    "            if(self.overlapDutyInfo[c] < self.minDutyCycle):\n",
    "                self.increase_Permanence(c)\n",
    "                #print(\"min\", self.minDutyCycle)\n",
    "\n",
    "                \n",
    "# ------------------------------------------------------------------------------------------ #\n",
    "# ------------------------------------------------------------------------------------------ #\n",
    "\n",
    "\n",
    "    def getActiveColumns(self):\n",
    "        return self.activeColumns\n",
    "                        \n",
    "                        \n",
    "    ''' global 하게 승리할 컬럼의 기준 '''\n",
    "    def kthScore(self, desired_kth):\n",
    "        \n",
    "        rank = self.overlapped.ravel().copy()\n",
    "        rank.sort()        \n",
    "        score = rank[-desired_kth]\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    \n",
    "    ''' global 하게 가장 자주 승리한 컬럼의 duty '''\n",
    "    def maxhighDutyCycle(self):\n",
    "        \n",
    "        rank = self.activeDutyInfo.ravel().copy()\n",
    "        rank.sort()\n",
    "        maxDuty = rank[-1]\n",
    "        highDuty = rank[-int(self.input_size/5)]\n",
    "        \n",
    "        return maxDuty, highDuty\n",
    "    \n",
    "\n",
    "    \n",
    "    ''' 해당 column 이 발화하도록 격려 '''\n",
    "    def boostFunction(self, c, boost):\n",
    "        if(self.activeDutyInfo[c] <= self.minDutyCycle):\n",
    "            self.boosts[c] += boost\n",
    "        elif(self.activeDutyInfo[c] > self.highDutyCycle):\n",
    "            self.boosts[c] -= boost\n",
    "            \n",
    "            \n",
    "    ''' 해당 column 의 모든 petential synapse 의 permanence 를 증가시켜 잘 겹치도록 격려 '''\n",
    "    def increase_Permanence(self, c):\n",
    "        for s in range(self.input_size):\n",
    "            if(self.potential_synapses[s, c] != -1):\n",
    "                self.potential_synapses[s, c] += self.permanence_inc\n",
    "        \n",
    "        \n",
    "    ''' activeDuty update '''\n",
    "    def update_activeDuty(self):\n",
    "        for c in range(self.columnCount):\n",
    "            self.activeDutyInfo[c] = np.mean(self.activeHistory[c])\n",
    "\n",
    "                \n",
    "    ''' overlapDuty update '''\n",
    "    def update_overlapDuty(self):\n",
    "        for c in range(self.columnCount):\n",
    "            self.overlapDutyInfo[c] = np.mean(self.overlapHistory[c])\n",
    "    \n",
    "    def visualize_SDR(self):\n",
    "        fig = plt.figure(figsize=(20,1))\n",
    "        plt.imshow(self.activeColumns.reshape(1, self.columnCount))\n",
    "        #plt.subplots_adjust(bottom=0.1, right=0.8, top=0.9)\n",
    "        #cax = plt.axes([0.85, 0.1, 0.075, 0.8])\n",
    "        #plt.colorbar(cax=cax)\n",
    "        #plt.show()\n",
    "        \n",
    "        sparsity = (np.count_nonzero(self.activeColumns==True)/(self.columnCount))\n",
    "        return sparsity\n",
    "    \n",
    "    def test(self):\n",
    "        idx = []\n",
    "        for i in range(len(self.activeColumns)):\n",
    "            if(self.activeColumns[i] == 1):\n",
    "                idx.append(i)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = SpatialPooler(32, 128, .8, 3, potential_rate=.7)\n",
    "se = Encoder.ScalarEncoder(out_size=32, max_val=30, w=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.380678653717041\n"
     ]
    }
   ],
   "source": [
    "step = 500\n",
    "encoded_data = 0\n",
    "\n",
    "start = time.time()\n",
    "for i in range(step):\n",
    "    rand = random.randint(0, se.max_val)\n",
    "    encoded_data = se.encode(rand)\n",
    "    sp.compute_SDR(encoded_data)\n",
    "    \n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 18, 43, 60, 91]\n",
      "[10, 11, 18, 43, 91]\n",
      "[12, 43, 83, 110, 117]\n",
      "[19, 43, 83, 110, 117]\n",
      "[19, 43, 83, 110, 117]\n",
      "[43, 56, 78, 110, 117]\n",
      "[16, 43, 71, 110, 117]\n",
      "[10, 11, 61, 62, 120]\n",
      "[10, 11, 61, 62, 120]\n",
      "[11, 51, 93, 110, 126]\n",
      "[10, 11, 38, 51, 126]\n",
      "[10, 11, 18, 38, 71]\n",
      "[10, 11, 18, 38, 71]\n",
      "[10, 11, 38, 94, 101]\n",
      "[10, 18, 47, 71, 122]\n",
      "[46, 101, 118, 122, 127]\n",
      "[10, 38, 99, 101, 118]\n",
      "[10, 38, 99, 101, 118]\n",
      "[10, 38, 42, 87, 101]\n",
      "[37, 38, 72, 87, 118]\n",
      "[28, 37, 87, 103, 112]\n",
      "[28, 37, 87, 103, 112]\n",
      "[37, 38, 58, 87, 97]\n",
      "[37, 68, 81, 97, 115]\n",
      "[0, 5, 22, 33, 37]\n",
      "[0, 5, 22, 33, 37]\n",
      "[0, 5, 22, 33, 37]\n",
      "[1, 37, 81, 97]\n",
      "[1, 37, 81, 97]\n",
      "[12, 43, 83, 110, 117]\n",
      "boost : \n",
      " [17.77 16.46 20.72 15.11 27.5  16.6  27.5  27.5  27.5  27.45 15.69 20.78\n",
      " 17.13 27.5  15.39 24.22 19.91 27.5  23.32 17.65 23.63 16.22 15.42 27.5\n",
      " 16.75 22.76 27.5  24.28 20.08 27.5  23.58 24.35 26.32 19.51 27.5  23.13\n",
      " 24.21 21.46 25.67 23.81 26.46 24.17 14.   23.52 20.55 27.5  17.99 21.78\n",
      " 27.5  23.71 20.43 21.27 24.46 27.5  27.5  22.99 19.51 23.62 14.22 24.16\n",
      " 23.92 23.87 14.86 27.5  26.36 27.5  26.5  27.5  23.02 27.5  27.5  16.77\n",
      " 17.06 14.51 14.18 26.32 27.5  27.5  24.35 24.01 27.5  14.61 27.49 17.41\n",
      " 26.5  27.5  27.5  17.71 16.71 23.44 27.5  16.05 19.46 16.97 15.46 27.5\n",
      " 24.01 16.3  23.8  20.98 27.5  20.81 27.5  15.07 15.05 26.5  27.5  27.5\n",
      " 27.5  24.22 17.65 27.5  15.16 27.5  14.08 23.02 24.21 16.13 17.43 27.5\n",
      " 13.71 22.91 21.36 23.49 27.5  17.04 18.46 21.35]\n",
      "active : \n",
      " [0.06 0.02 0.   0.01 0.   0.03 0.   0.   0.   0.   0.47 0.3  0.05 0.\n",
      " 0.1  0.   0.04 0.   0.2  0.04 0.   0.09 0.06 0.   0.07 0.01 0.   0.03\n",
      " 0.04 0.   0.   0.   0.   0.06 0.   0.   0.   0.28 0.35 0.   0.   0.\n",
      " 0.02 0.19 0.   0.   0.01 0.02 0.   0.   0.   0.02 0.   0.   0.   0.\n",
      " 0.04 0.   0.14 0.   0.03 0.02 0.06 0.   0.   0.   0.   0.   0.01 0.\n",
      " 0.   0.22 0.01 0.   0.06 0.   0.   0.   0.03 0.   0.   0.04 0.   0.07\n",
      " 0.   0.   0.   0.19 0.02 0.   0.   0.13 0.   0.01 0.08 0.   0.   0.22\n",
      " 0.   0.05 0.   0.26 0.   0.02 0.04 0.   0.   0.   0.   0.   0.17 0.\n",
      " 0.02 0.   0.   0.01 0.   0.19 0.14 0.   0.05 0.   0.03 0.   0.   0.\n",
      " 0.03 0.01]\n",
      "overlap : \n",
      " [0.52 0.69 0.82 1.   0.23 0.41 0.18 0.25 0.31 0.1  1.   0.78 0.94 0.28\n",
      " 1.   0.46 0.46 0.23 0.83 0.73 0.87 0.92 0.94 0.39 1.   0.33 0.28 0.2\n",
      " 0.62 0.14 0.54 0.38 0.2  0.27 0.56 0.35 0.29 0.74 0.7  0.77 0.26 0.49\n",
      " 1.   0.58 0.6  0.31 0.93 0.53 0.25 0.73 0.7  0.68 0.5  0.35 0.27 0.18\n",
      " 0.57 0.56 1.   0.37 0.54 0.52 1.   0.48 0.39 0.26 0.23 0.29 0.24 0.31\n",
      " 0.29 1.   1.   0.94 0.92 0.36 0.32 0.27 0.57 0.3  0.43 1.   0.37 0.94\n",
      " 0.4  0.29 0.18 1.   0.92 0.24 0.38 1.   0.62 1.   0.94 0.37 0.36 0.92\n",
      " 0.4  0.44 0.63 0.65 0.4  0.84 1.   0.26 0.15 0.44 0.18 0.4  1.   0.52\n",
      " 1.   0.4  1.   0.4  0.36 1.   1.   0.36 1.   0.28 0.54 0.3  0.1  0.82\n",
      " 0.72 0.41]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1, se.max_val):\n",
    "    sp.compute_SDR(se.encode(i))\n",
    "    #sp.visualize_SDR()\n",
    "    print(sp.test())\n",
    "    \n",
    "sp.compute_SDR(se.encode(3))\n",
    "sp.visualize_SDR()\n",
    "\n",
    "print(sp.test())\n",
    "print(\"boost : \\n\", sp.boosts)\n",
    "print(\"active : \\n\", sp.activeDutyInfo)\n",
    "print(\"overlap : \\n\", sp.overlapDutyInfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\matplotlib\\pyplot.py:522: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boost : \n",
      " [1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 1. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 1.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 2. 1. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]\n",
      "active : \n",
      " [0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.01\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.\n",
      " 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.01 0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01\n",
      " 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.01 0.   0.   0.\n",
      " 0.   0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.  ]\n",
      "overlap : \n",
      " [0.09 0.99 0.99 0.82 0.98 0.99 0.99 0.99 0.99 0.09 0.99 0.83 0.08 0.99\n",
      " 0.82 0.5  0.98 0.99 0.2  0.99 0.98 0.08 0.98 0.99 0.42 0.75 0.68 0.43\n",
      " 0.99 0.75 0.25 0.99 0.99 0.83 0.98 0.48 0.99 0.79 0.81 0.99 0.74 0.98\n",
      " 0.98 0.98 0.88 0.98 0.83 0.99 0.88 0.98 0.09 0.73 0.54 0.98 0.99 0.99\n",
      " 0.99 0.75 0.52 0.99 0.3  0.98 0.98 0.99 0.99 0.77 0.11 0.98 0.99 0.45\n",
      " 0.85 0.47 0.52 0.77 0.99 0.35 0.99 0.09 0.99 0.97 0.99 0.98 0.98 0.98\n",
      " 0.99 0.56 0.78 0.98 0.99 0.98 0.99 0.99 0.55 0.43 0.99 0.76 0.26 0.99\n",
      " 0.73 0.99 0.47 0.73 0.99 0.77 0.08 0.19 0.99 0.31 0.99 0.24 0.98 0.78\n",
      " 0.99 0.98 0.98 0.99 0.23 0.48 0.48 0.99 0.59 0.99 0.99 0.19 0.81 0.09\n",
      " 0.99 0.84 0.23 0.08 0.4  0.99 0.99 0.99 0.99 0.82 0.98 0.72 0.99 0.99\n",
      " 0.75 0.48 0.85 0.25 0.41 0.99 0.99 0.84 0.97 0.27 0.41 0.83 0.98 0.99\n",
      " 0.73 0.98 0.99 0.99 0.98 0.99 0.74 0.02 0.79 0.99 0.99 0.78 0.99 0.19\n",
      " 0.79 0.78 0.08 0.99 0.99 0.45 0.85 0.98 0.09 0.73 0.99 0.45 0.99 0.99\n",
      " 0.99 0.98 0.99 0.98 0.74 0.98 0.73 0.99 0.7  0.73 0.99 0.77 0.99 0.74\n",
      " 0.99 0.98 0.99 0.98 0.99 0.99 0.98 0.98 0.98 0.09 0.99 0.99 0.75 0.82\n",
      " 0.99 0.26 0.98 0.05 0.98 0.99 0.98 0.76 0.99 0.81 0.99 0.99 0.99 0.99\n",
      " 0.52 0.23 0.98 0.99 0.51 0.5  0.75 0.73 0.75 0.98 0.98 0.75 0.98 0.26\n",
      " 0.77 0.31 0.99 0.88 0.98 0.81 0.99 0.52 0.97 0.83 0.45 0.98 0.33 0.59\n",
      " 0.25 0.99 0.98 0.73 0.99 0.99 0.99 0.81 0.99 0.09 0.98 0.99 0.88 0.99\n",
      " 0.31 0.99 0.75 0.23 0.23 0.99 0.51 0.98 0.99 0.99 0.99 0.98 0.98 0.99\n",
      " 0.73 0.99 0.99 0.99 0.78 0.99 0.99 0.49 0.98 0.99 0.41 0.99 0.77 0.99\n",
      " 0.98 0.98 0.99 0.99 0.7  0.99 0.99 0.74 0.73 0.81 0.99 0.99 0.98 0.99\n",
      " 0.54 0.83 0.83 0.44 0.98 0.78 0.98 0.09 0.85 0.99 0.83 0.42 0.26 0.98\n",
      " 0.99 0.61 0.81 0.99 0.99 0.99 0.99 0.51 0.43 0.81 0.99 0.99 0.99 0.99\n",
      " 0.82 0.64 0.99 0.98 0.98 0.51 0.73 0.53 0.99 0.49 0.99 0.73 1.   0.99\n",
      " 0.4  0.99 0.99 0.99 0.99 0.98 0.99 0.23 0.23 0.99 0.64 0.74 0.99 0.77\n",
      " 0.98 0.99 0.56 0.99 0.99 0.21 0.25 0.85 0.99 0.79 0.99 0.99 0.99 0.11\n",
      " 0.49 0.98 0.99 0.72 0.31 0.99 0.73 0.99 0.99 0.99 0.51 0.99 0.98 0.85\n",
      " 0.99 0.98 0.78 0.98 0.78 0.72 0.68 0.51 0.88 0.55 0.99 0.74 1.   0.81\n",
      " 0.98 0.41 0.99 0.99 0.99 0.75 0.99 0.88 0.17 0.98 0.99 0.99 0.54 0.98\n",
      " 0.98 0.99 0.99 0.99 0.99 0.51 0.99 0.98 0.56 0.79 0.55 0.98 0.23 0.98\n",
      " 0.72 0.99 0.99 0.99 0.7  0.99 0.82 0.51 0.98 0.98 0.99 0.99 0.98 0.99\n",
      " 0.99 0.99 0.75 0.99 0.99 0.98 0.75 0.98 0.99 0.99 0.02 0.99 0.51 0.99\n",
      " 0.98 0.99 0.99 0.99 0.99 0.52 0.85 0.99 0.03 1.   0.99 0.99 0.98 0.77\n",
      " 0.99 0.99 0.78 0.44 0.28 0.98 0.54 0.98 0.99 0.99 0.99 0.2  0.73 1.\n",
      " 0.99 0.75 0.98 0.98 0.99 0.99 0.78 0.09 0.98 0.83 0.99 0.74 0.86 0.74\n",
      " 0.21 0.99 0.25 0.48 0.99 0.19 0.99 0.23 0.99 0.73 0.54 0.77 0.99 0.99\n",
      " 0.99 0.99 0.48 0.83 0.99 0.99 0.99 0.99 0.85 0.98 0.81 0.99 0.98 0.45\n",
      " 0.99 0.85 0.99 0.99 0.99 0.82 0.82 0.99 0.33 0.26 0.99 0.77 0.99 0.98\n",
      " 0.98 0.78 0.99 0.46 0.78 0.99 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99\n",
      " 0.98 0.44 0.99 0.99 0.99 0.99 0.98 0.52 0.25 0.22 0.03 0.55 0.99 0.99\n",
      " 0.4  0.75 0.48 0.23 0.98 0.99 0.99 0.77 0.98 0.99 0.98 0.48 0.99 0.99\n",
      " 0.98 0.98 0.28 0.75 0.99 0.99 0.2  0.99 0.48 0.99 0.99 0.81 0.99 0.98\n",
      " 0.26 0.73 0.99 0.79 0.12 0.75 0.69 0.83 0.99 0.38 0.98 0.48 0.99 0.77\n",
      " 0.98 0.99 0.26 0.64 0.98 0.22 0.98 0.74 0.23 0.99 0.75 0.99 0.73 0.99\n",
      " 0.99 0.73 0.99 0.83 0.99 0.99 0.98 0.98 0.99 1.   0.98 0.98 0.99 0.98\n",
      " 0.99 0.99 0.52 0.99 0.81 0.99 0.98 0.99 0.99 0.78 0.69 0.99 0.99 0.82\n",
      " 0.99 0.78 0.98 0.99 0.98 0.99 0.88 0.98 0.99 0.46 0.21 0.27 0.76 0.82\n",
      " 0.49 0.22 0.99 0.99 0.98 0.99 0.73 0.7  0.99 0.76 0.99 0.99 0.15 0.78\n",
      " 0.98 0.99 0.99 0.25 0.99 0.99 0.98 0.85 0.53 0.98 0.73 0.99 0.99 0.99\n",
      " 0.75 0.78 0.77 0.54 0.99 0.99 0.99 0.52 0.98 0.98 0.99 0.99 0.58 0.78\n",
      " 0.99 0.99 0.74 0.75 0.99 0.99 0.75 0.99 0.99 0.25 0.99 0.99 0.99 0.99\n",
      " 0.99 0.99 0.99 0.99 0.98 0.99 0.58 0.09 0.98 0.75 0.99 0.02 0.99 0.99\n",
      " 0.81 0.98 0.68 0.23 0.73 0.85 0.98 0.74 0.99 0.23 0.99 0.84 0.99 0.99\n",
      " 0.53 0.99 0.47 0.99 0.99 0.99 0.09 0.99 0.99 0.26 0.98 0.99 0.99 0.88\n",
      " 0.98 0.85 0.98 0.79 0.98 0.35 0.44 0.98 0.98 0.99 0.43 0.99 0.99 0.7\n",
      " 0.19 0.78 0.99 0.74 0.99 0.21 0.98 0.09 0.74 0.99 0.99 0.98 0.39 0.99\n",
      " 0.38 0.98 0.73 0.99 0.73 0.09 0.33 0.99 0.24 0.99 0.83 0.99 0.72 0.99\n",
      " 0.99 0.56 0.77 0.99 0.46 0.99 0.75 0.78 0.99 0.99 0.99 0.98 0.98 0.77\n",
      " 0.99 0.99 0.2  0.98 0.78 0.79 0.99 0.99 0.99 0.99 0.64 0.21 0.99 0.72\n",
      " 0.99 0.52 0.99 0.98 0.06 0.98 0.99 0.99 0.03 0.09 0.77 0.99 0.44 0.98\n",
      " 0.98 0.81 0.99 0.99 0.99 0.98 0.99 0.79 0.25 0.99 0.99 0.98 0.99 0.99\n",
      " 0.99 0.88 0.8  0.77 0.99 0.55 0.05 0.98 0.75 0.98 0.99 0.98 0.99 0.2\n",
      " 0.09 0.99 0.99 0.27 0.47 0.85 0.99 0.98 0.98 0.99 0.77 0.73 0.55 0.99\n",
      " 0.77 0.81 0.98 0.98 0.07 0.99 0.52 0.99 0.98 0.98 0.98 0.99 0.99 0.99\n",
      " 0.09 0.79 0.98 0.46 0.83 0.99 0.99 0.99 0.98 0.99 0.75 0.98 0.99 0.51\n",
      " 0.99 0.99 0.99 0.85 0.99 0.99 0.99 0.98 0.99 0.99 0.98 0.78 0.26 0.75\n",
      " 0.99 0.99 0.98 0.74 0.99 0.99 0.77 0.85 0.98 0.56 0.85 0.81 0.99 0.83\n",
      " 0.51 0.09 0.99 0.98 0.82 0.99 0.74 0.57 0.99 0.82 0.44 0.81 0.99 0.76\n",
      " 0.99 0.99 0.98 0.98 0.77 0.98 0.48 0.75 0.99 0.61 0.19 0.99 0.81 0.98\n",
      " 0.75 0.51 0.4  0.99 0.99 0.74 0.77 0.99 0.98 0.99 0.75 0.42 0.99 0.72\n",
      " 0.99 0.98 0.99 0.77 0.49 0.99 0.72 0.73 0.23 0.83 0.98 0.98 0.74 0.77\n",
      " 0.81 0.99 0.99 0.99 0.98 0.97 0.99 0.99 0.99 0.99 0.99 0.98 0.99 0.99\n",
      " 0.73 0.98]\n",
      "--------------------------------------------------\n",
      "\n",
      "\n",
      "sparsity 평균 : 0.0002734375, 표준편차 : 0.0027206687733384136\n",
      "\n",
      "activeDuty 평균 : 0.0002734375, 표준편차 : 0.0016308301363396958\n",
      "\n",
      "overlap 평균 : 0.8060742187500001, 표준편차 : 0.2650630422495319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5. 제대로 된 data 주입해보기\n",
    "\n",
    "import random\n",
    "\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "sp = SpatialPooler(10,1024,.8,3)\n",
    "        \n",
    "step = 100\n",
    "on_cnt = 5\n",
    "rand_data = np.zeros([10])\n",
    "\n",
    "y=np.empty([step])\n",
    "\n",
    "for s in range(step):\n",
    "    rand_data = np.zeros([10])\n",
    "    \n",
    "    for i in range(on_cnt):\n",
    "        idx = random.randint(0,9)\n",
    "        \n",
    "        while(True):\n",
    "            if(rand_data[idx] == 1):\n",
    "                idx = random.randint(0,9)\n",
    "            else:\n",
    "                break\n",
    "                \n",
    "        rand_data[idx] = 1\n",
    "        \n",
    "    sp.compute_SDR(rand_data)\n",
    "    y[s] = sp.visualize_SDR()\n",
    "    #y.append(sp.visualize_SDR())\n",
    "    \n",
    "plt.plot(range(step), y)\n",
    "\n",
    "print(\"boost : \\n\", sp.boosts)\n",
    "print(\"active : \\n\", sp.activeDutyInfo)\n",
    "print(\"overlap : \\n\", sp.overlapDutyInfo)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"\\n\")\n",
    "print(\"sparsity 평균 : {}, 표준편차 : {}\\n\".format(np.mean(y), np.std(y)))\n",
    "print(\"activeDuty 평균 : {}, 표준편차 : {}\\n\".format(np.mean(sp.activeDutyInfo), np.std(sp.activeDutyInfo)))\n",
    "print(\"overlap 평균 : {}, 표준편차 : {}\\n\".format(np.mean(sp.overlapDutyInfo), np.std(sp.overlapDutyInfo)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
